import os
import json
import time
from typing import Any, Dict, List, Optional
import sys
from pathlib import Path

from supabase import create_client, Client
from openai import OpenAI

# backend í´ë”ë¥¼ pathì— ë¨¼ì € ì¶”ê°€ (venvì˜ config íŒ¨í‚¤ì§€ë³´ë‹¤ ìš°ì„ )
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from config import settings

# =========================
# ì„¤ì •
# =========================
EMBEDDING_MODEL = "text-embedding-3-small"  # ë³´í†µ 1536-dim
EMBEDDING_DIM = 1536                       # products_vectorì˜ vector(n)ê³¼ ì¼ì¹˜í•´ì•¼ í•¨

PAGE_SIZE = 200                            # productsì—ì„œ ì½ì–´ì˜¤ëŠ” ë‹¨ìœ„
EMBED_BATCH_SIZE = 100                     # OpenAI ì„ë² ë”© ìš”ì²­ ë°°ì¹˜ í¬ê¸°
SLEEP_BETWEEN_PAGES = 0.1                  # ë„ˆë¬´ ë¹ ë¥´ë©´ DB/ë„¤íŠ¸ì›Œí¬ ë¶€ë‹´ë  ìˆ˜ ìˆì–´ ì•½ê°„ ì‰¬ê¸°

# products_vector í…Œì´ë¸”ì˜ PK ì»¬ëŸ¼ëª… (ë³´í†µ product_id)
VECTOR_PK_COL = "product_id"

# =========================
# ìœ í‹¸
# =========================
def safe_json_parse(value: Any, fallback: Any):
    """features/analytics/keywordsê°€ str(JSON)ì¼ ìˆ˜ë„ ìˆê³  ì´ë¯¸ dict/listì¼ ìˆ˜ë„ ìˆì–´ì„œ ì•ˆì „ íŒŒì‹±."""
    if value is None:
        return fallback
    if isinstance(value, (dict, list)):
        return value
    if isinstance(value, str):
        try:
            return json.loads(value)
        except Exception:
            return fallback
    return fallback


def pick_arrays(obj: Dict[str, Any], keys: List[str]) -> List[str]:
    out = []
    for k in keys:
        v = obj.get(k)
        if isinstance(v, list):
            out.extend([str(x) for x in v if x])
    return out


def build_embedding_text(p: Dict[str, Any]) -> str:
    """
    ì œí’ˆ row -> ì„ë² ë”©ìš© ìì—°ì–´ content ìƒì„±
    (ì˜ë¯¸/ì·¨í–¥ ìœ ì‚¬ë„ì— ë„ì›€ ë˜ëŠ” í…ìŠ¤íŠ¸ë§Œ ìµœëŒ€í•œ ëª¨ìŒ)
    """
    features = safe_json_parse(p.get("features"), {})
    keywords = safe_json_parse(p.get("keywords"), [])
    analytics = safe_json_parse(p.get("analytics"), {})

    # featuresì—ì„œ í…ìŠ¤íŠ¸ë¡œ ì˜ë¯¸ ìˆëŠ” ê²ƒë“¤ë§Œ ì­‰ ëª¨ìœ¼ê¸°
    feature_lines = []
    feature_lines += pick_arrays(features, ["key_points", "visual_concept", "technology_ingredients"])
    feature_lines += pick_arrays(features, ["efficacy_data", "reliability"])
    feature_lines += pick_arrays(features, ["emotional_benefits"])
    feature_lines += pick_arrays(features, ["texture", "scent"])
    feature_lines += pick_arrays(features, ["usage"])

    analytics_lines = []
    if analytics.get("skin_type"):
        analytics_lines.append(f"í”¼ë¶€íƒ€ì…: {analytics['skin_type']}")
    if analytics.get("age_group"):
        analytics_lines.append(f"ì—°ë ¹ëŒ€: {analytics['age_group']}")

    category_path = " > ".join(
        [x for x in [p.get("category_major"), p.get("category_middle"), p.get("category_small")] if x]
    )

    lines = [
        f"ë¸Œëœë“œ: {p.get('brand', '')}",
        f"ì œí’ˆëª…: {p.get('name', '')}",
        f"ì¹´í…Œê³ ë¦¬: {category_path}" if category_path else None,
        f"í‚¤ì›Œë“œ: {', '.join(keywords)}" if isinstance(keywords, list) and len(keywords) else None,
        f"íŠ¹ì§•: {' | '.join(feature_lines)}" if len(feature_lines) else None,
        f"íƒ€ê²Ÿ/í”¼ë¶€ì •ë³´: {', '.join(analytics_lines)}" if len(analytics_lines) else None,
    ]

    # None ì œê±° í›„ í•©ì¹˜ê¸°
    return "\n".join([x for x in lines if x])


def build_metadata(p: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë²¡í„° ê²€ìƒ‰ ì´í›„ ë£°ë² ì´ìŠ¤ í•„í„°/ì •ë ¬/ì¬ë­í‚¹ì— ìœ ìš©í•œ ìˆ«ì/ì¹´í…Œê³ ë¦¬ ì €ì¥.
    """
    return {
        "brand": p.get("brand"),
        "category_major": p.get("category_major"),
        "category_middle": p.get("category_middle"),
        "category_small": p.get("category_small"),
        "price_final": p.get("price_final"),
        "discount_rate": p.get("discount_rate"),
        "review_score": p.get("review_score"),
        "review_count": p.get("review_count"),
        "benefit_discount_rate": p.get("benefit_discount_rate"),
        "price_benefit": p.get("price_benefit"),
    }


def chunk_list(items: List[Any], size: int) -> List[List[Any]]:
    return [items[i:i + size] for i in range(0, len(items), size)]


def embed_texts(openai_client: OpenAI, texts: List[str], max_retries: int = 5) -> List[List[float]]:
    """
    OpenAI embeddings í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)
    """
    for attempt in range(1, max_retries + 1):
        try:
            res = openai_client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=texts,
                encoding_format="float",
                # dimensions=768,  # ì°¨ì› ì¤„ì´ë©´ DB vector(n)ë„ nì— ë§ê²Œ ë°”ê¿”ì•¼ í•¨
            )
            vectors = [d.embedding for d in res.data]
            return vectors
        except Exception as e:
            if attempt == max_retries:
                raise
            wait = 2 ** attempt
            print(f"[WARN] embedding ì‹¤íŒ¨, {wait}s í›„ ì¬ì‹œë„ ({attempt}/{max_retries}) - {e}")
            time.sleep(wait)

    raise RuntimeError("embedding ì¬ì‹œë„ ì‹¤íŒ¨")


# =========================
# ë©”ì¸ ë¡œì§
# =========================
def main():
    supabase_url = settings.SUPABASE_URL
    supabase_key = settings.SUPABASE_KEY
    openai_key = settings.openai_api_key

    sb: Client = create_client(supabase_url, supabase_key)
    oa = OpenAI(api_key=openai_key)

    offset = 0
    total_processed = 0

    while True:
        # 1) products í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì½ê¸°
        resp = (
            sb.table("products")
            .select(
                "id, product_code, brand, name, category_major, category_middle, category_small,"
                "features, analytics, keywords,"
                "price_original, price_final, discount_rate, review_score, review_count,"
                "price_benefit, benefit_discount_rate"
            )
            .range(offset, offset + PAGE_SIZE - 1)
            .execute()
        )

        products = resp.data or []
        if not products:
            print("âœ… ëª¨ë“  products ì²˜ë¦¬ ì™„ë£Œ")
            break

        # 2) content ë§Œë“¤ê¸°
        contents = [build_embedding_text(p) for p in products]

        # 3) ì„ë² ë”©ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ í˜¸ì¶œ
        all_vectors: List[List[float]] = []
        for batch in chunk_list(contents, EMBED_BATCH_SIZE):
            vectors = embed_texts(oa, batch)
            all_vectors.extend(vectors)

        # 4) upsert payload êµ¬ì„±
        upserts = []
        for p, content, emb in zip(products, contents, all_vectors):
            if len(emb) != EMBEDDING_DIM:
                raise ValueError(f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: got {len(emb)}, expected {EMBEDDING_DIM}")

            upserts.append({
                VECTOR_PK_COL: p["id"],          # products.id -> products_vector.product_id
                "content": content,
                "embedding": emb,               # vector ì»¬ëŸ¼ì— list[float] ë„£ê¸°
                "metadata": build_metadata(p),
            })

        # 5) products_vectorì— upsert
        #    on_conflictëŠ” PK ì»¬ëŸ¼ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨
        upsert_resp = (
            sb.table("products_vector")
            .upsert(upserts, on_conflict=VECTOR_PK_COL)
            .execute()
        )

        total_processed += len(products)
        offset += PAGE_SIZE

        print(f"âœ… upsert ì™„ë£Œ: ì´ë²ˆ {len(products)}ê°œ / ëˆ„ì  {total_processed}ê°œ")

        time.sleep(SLEEP_BETWEEN_PAGES)

    print("ğŸ‰ ì „ì²´ ì„ë² ë”© ì ì¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()
