import os
import json
from typing import Dict, Any, List, Optional, Tuple
from openai import OpenAI
import httpx
from config import settings
from supabase import create_client
from sentence_transformers import CrossEncoder
import torch

# Cross-Encoder ì„¤ì •
TOP_K = 3
CANDIDATE_POOL = 200
EMBED_MODEL = "text-embedding-3-small"
EMBED_DIM = 1536
CE_MODEL = "BAAI/bge-reranker-v2-m3"
KW_BONUS_ALPHA = 1.2
CUSTOMER_ID_COL = "user_id"
PRODUCT_VECTOR_FK_COL = "product_id"

# ë™ì˜ì–´ ë§¤í•‘
SKIN_TYPE_MAP = {
    "Sensitive": "ë¯¼ê°ì„±",
    "Dry": "ê±´ì„±",
    "Oily": "ì§€ì„±",
    "Combination": "ë³µí•©ì„±",
    "Neutral": "ì¤‘ì„±",
    "Normal": "ì¤‘ì„±",
}

CONCERN_MAP = {
    "Pores": "ëª¨ê³µ",
    "Sebum": "í”¼ì§€",
    "Acne": "ì—¬ë“œë¦„",
    "Redness": "í™ì¡°",
    "Dryness": "ê±´ì¡°",
    "Wrinkle": "ì£¼ë¦„",
    "Elasticity": "íƒ„ë ¥",
    "Dullness": "ì¹™ì¹™í•¨",
    "Anti-aging": "ì•ˆí‹°ì—ì´ì§•",
    "Antiaging": "ì•ˆí‹°ì—ì´ì§•",
    "Sensitive": "ë¯¼ê°",
    "Sensitivity": "ë¯¼ê°",
}

TONE_MAP = {
    "Cool_Summer": "ì¿¨í†¤ ì—¬ë¦„",
    "Cool_Winter": "ì¿¨í†¤ ê²¨ìš¸",
    "Warm_Spring": "ì›œí†¤ ë´„",
    "Warm_Autumn": "ì›œí†¤ ê°€ì„",
    "Neutral": "ë‰´íŠ¸ëŸ´",
}

# Cross-Encoder ìºì‹± (í•œ ë²ˆë§Œ ë¡œë“œ)
_cross_encoder_cache = None

client = OpenAI(api_key=settings.OPENAI_API_KEY)

def get_cross_encoder() -> CrossEncoder:
    """Cross-Encoderë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _cross_encoder_cache
    if _cross_encoder_cache is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[CrossEncoder] loading: {CE_MODEL} on device={device}")
        _cross_encoder_cache = CrossEncoder(CE_MODEL, device=device)
    return _cross_encoder_cache


def normalize_list(v: Any) -> List[str]:
    """ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”"""
    if v is None:
        return []
    if isinstance(v, list):
        return [str(x).strip() for x in v if str(x).strip()]
    if isinstance(v, str):
        s = v.strip()
        if s.startswith("{") and s.endswith("}"):
            s = s[1:-1]
        return [x.strip().strip('"') for x in s.split(",") if x.strip()]
    return [str(v).strip()]


def with_kr(items: List[str], mapping: Dict[str, str]) -> List[str]:
    """ì˜ì–´ í‚¤ì›Œë“œì— í•œê¸€ ë§¤í•‘ ì¶”ê°€"""
    out = []
    for x in items:
        k = mapping.get(x)
        out.append(f"{x}({k})" if k else x)
    return out


def build_user_query_text(customer: Dict[str, Any]) -> str:
    """ìœ ì € ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±"""
    skin_types = with_kr(normalize_list(customer.get("skin_type")), SKIN_TYPE_MAP)
    concerns = with_kr(normalize_list(customer.get("skin_concerns")), CONCERN_MAP)
    keywords = normalize_list(customer.get("keywords"))
    tone = customer.get("preferred_tone")
    tone_kr = TONE_MAP.get(tone, tone) if tone else None

    lines = [
        "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ ì¶”ì²œ ì¿¼ë¦¬ (í‚¤ì›Œë“œ ìµœìš°ì„ )",
        "",
        "[ì¤‘ìš” í‚¤ì›Œë“œ TOP - ìµœìš°ì„  ë°˜ì˜]",
        f"- {', '.join(keywords)}" if keywords else "- (ì—†ìŒ)",
        "â€» ìœ„ í‚¤ì›Œë“œì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ” íš¨ëŠ¥/íŠ¹ì§•/ì œí’ˆ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì œí’ˆì„ ìµœìš°ì„ ìœ¼ë¡œ í‰ê°€í•œë‹¤.",
        "",
        "[í”¼ë¶€íƒ€ì…]",
        f"- {', '.join(skin_types)}" if skin_types else "- ì •ë³´ ì—†ìŒ",
        "",
        "[í”¼ë¶€ê³ ë¯¼]",
        f"- {', '.join(concerns)}" if concerns else "- ì •ë³´ ì—†ìŒ",
        "",
        "[ì¶”êµ¬ í†¤]",
        f"- {tone_kr}" if tone_kr else "- ì •ë³´ ì—†ìŒ",
        "",
        "[í‰ê°€ ê¸°ì¤€ ì¬ê°•ì¡°]",
        f"- í•µì‹¬ í‚¤ì›Œë“œ({', '.join(keywords)})ì™€ ì—°ê´€ì„±ì´ ë†’ì€ ì œí’ˆì„ ìš°ì„  ì¶”ì²œ" if keywords else "- í‚¤ì›Œë“œ ê¸°ë°˜ ìš°ì„  ì¶”ì²œ",
    ]
    return "\n".join(lines)


def embed_text(oa: OpenAI, text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    res = oa.embeddings.create(
        model=EMBED_MODEL,
        input=[text],
        encoding_format="float",
    )
    emb = res.data[0].embedding
    if len(emb) != EMBED_DIM:
        raise ValueError(f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: got {len(emb)} expected {EMBED_DIM}")
    return emb


def truncate_for_ce(text: str, max_chars: int = 1800) -> str:
    """Cross-Encoder ì…ë ¥ ê¸¸ì´ ì œí•œ"""
    text = text or ""
    return text if len(text) <= max_chars else text[:max_chars]


def keyword_bonus(user_keywords: List[str], product_content: str) -> float:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚° (0~1)"""
    kws = [k.strip() for k in (user_keywords or []) if k and str(k).strip()]
    if not kws:
        return 0.0

    text = (product_content or "").lower()
    keyword_line = ""
    for line in (product_content or "").splitlines():
        if "í‚¤ì›Œë“œ" in line:
            keyword_line = line.lower()
            break

    hit_any = 0
    hit_kwline = 0
    for kw in kws:
        k = kw.lower()
        if k in text:
            hit_any += 1
        if keyword_line and k in keyword_line:
            hit_kwline += 1

    score = (2.0 * hit_kwline + 1.0 * hit_any) / (3.0 * max(len(kws), 1))
    return float(min(1.0, max(0.0, score)))


async def fetch_products_from_supabase() -> Dict[str, str]:
    """
    Fetch products from Supabase and format them for the LLM.
    (Deprecated - ì´ì œ recommend_product_with_brands ì‚¬ìš©)
    """
    return {}

    url = f"{settings.SUPABASE_URL}/rest/v1/products"
    headers = {
        "apikey": settings.SUPABASE_KEY,
        "Authorization": f"Bearer {settings.SUPABASE_KEY}",
    }

    try:
        async with httpx.AsyncClient() as http_client:
            response = await http_client.get(url, headers=headers)
            response.raise_for_status()
            products_data = response.json()
            
            # Format: "ID": "Name (Brand, Category, Description)"
            formatted_products = {}
            full_data = {}  # Store full product data
            for p in products_data:
                # Adjust field names based on actual DB schema
                # Schema: id, product_code, brand, name, category_major, category_middle, category_small, 
                # price_original, price_final, discount_rate, review_score, review_count, features, analytics, keywords
                
                p_id = p.get("product_code") or str(p.get("id"))
                name = p.get("name")
                brand = p.get("brand", "")
                
                # Construct category string
                cats = [p.get("category_major"), p.get("category_middle"), p.get("category_small")]
                category = " > ".join([c for c in cats if c])
                
                # Construct description from keywords and features
                keywords = p.get("keywords", "")
                price = p.get("price_final")
                review_score = p.get("review_score")
                
                desc_parts = []
                if keywords:
                    desc_parts.append(f"Keywords: {keywords}")
                if price:
                    desc_parts.append(f"Price: {price}")
                if review_score:
                    desc_parts.append(f"Rating: {review_score}")
                
                desc = ", ".join(desc_parts)
                
                if p_id and name:
                    info = f"{name} (Brand: {brand}, Category: {category}, {desc})"
                    formatted_products[p_id] = info
                    # Store full product data
                    full_data[p_id] = p
            
            PRODUCTS_CACHE = formatted_products
            PRODUCTS_FULL_DATA = full_data
            
            # Debug: Print first 3 products to verify format
            print("DEBUG: Sample products from DB:")
            for i, (pid, info) in enumerate(formatted_products.items()):
                if i >= 3: break
                print(f" - {pid}: {info}")
                
            return formatted_products
            
    except Exception as e:
        print(f"Failed to fetch products from Supabase: {e}")
        # Fallback to empty dict or hardcoded list if needed
        return {}

async def recommend_product_with_brands(
    user_id: str,
    user_data: Any,
    target_brands: List[str] = None,
    top_k: int = 1
) -> Optional[Dict[str, Any]]:
    """
    ìœ ì € IDì™€ ë¸Œëœë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ Cross-Encoder ê¸°ë°˜ìœ¼ë¡œ ìµœê³ ì˜ ìƒí’ˆì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    
    Args:
        user_id: ì‚¬ìš©ì ID
        user_data: CustomerProfile ê°ì²´ (user_dataì—ì„œ ì¶”ì¶œí•œ ì •ë³´)
        target_brands: ì¶”ì²œí•  ë¸Œëœë“œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ë¸Œëœë“œ)
        top_k: ë°˜í™˜í•  ìƒí’ˆ ê°œìˆ˜ (ê¸°ë³¸ê°’: 1)
        
    Returns:
        ì¶”ì²œ ìƒí’ˆ ì •ë³´ dict ë˜ëŠ” None
    """
    try:
        # Supabase ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        sb = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY)
        oa = OpenAI(api_key=settings.OPENAI_API_KEY)
        ce = get_cross_encoder()
        
        # 1) ê³ ê° ì •ë³´ ì¡°íšŒ
        customer_resp = (
            sb.table("customers")
            .select("user_id, skin_type, skin_concerns, keywords, preferred_tone")
            .eq(CUSTOMER_ID_COL, user_id)
            .limit(1)
            .execute()
        )
        
        if not customer_resp.data:
            print(f"[WARN] customersì—ì„œ {CUSTOMER_ID_COL}={user_id}ë¥¼ ì°¾ì§€ ëª»í•¨")
            return None
        
        customer = customer_resp.data[0]
        user_keywords = normalize_list(customer.get("keywords"))
        
        # 2) ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
        query_text = build_user_query_text(customer)
        
        # 3) ì„ë² ë”© ìƒì„±
        query_emb = embed_text(oa, query_text)
        
        # 4) ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (í›„ë³´ í’€)
        rpc_payload = {
            "filter": {},
            "match_count": CANDIDATE_POOL,
            "query_embedding": query_emb,
        }
        match_resp = sb.rpc("match_products", rpc_payload).execute()
        matches = match_resp.data or []
        
        if not matches:
            print("[WARN] ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        matches.sort(key=lambda m: float(m.get("similarity", 0.0)), reverse=True)
        candidate_ids = [m["product_id"] for m in matches]
        sim_map = {m["product_id"]: float(m["similarity"]) for m in matches}
        
        # 5) products ìƒì„¸ ì •ë³´ ì¡°íšŒ (ë¸Œëœë“œ í•„í„°ë§ ì ìš©)
        query = (
            sb.table("products")
            .select("id, brand, name, category_major, category_middle, category_small, price_final, discount_rate, review_score, review_count")
            .in_("id", candidate_ids)
        )
        
        if target_brands and len(target_brands) > 0:
            query = query.in_("brand", target_brands)
            print(f"  ğŸ·ï¸ ë¸Œëœë“œ í•„í„°ë§ ì ìš©: {target_brands}")
        
        products_resp = query.execute()
        products = products_resp.data or []
        
        if not products:
            if target_brands:
                print(f"[WARN] ì§€ì •ëœ ë¸Œëœë“œ({target_brands})ì—ì„œ ìƒí’ˆì„ ì°¾ì§€ ëª»í•¨")
            else:
                print("[WARN] ìƒí’ˆì„ ì°¾ì§€ ëª»í•¨")
            return None
        
        prod_map = {p["id"]: p for p in products}
        filtered_ids = list(prod_map.keys())
        
        # 6) products_vector content ê°€ì ¸ì˜¤ê¸°
        pv_resp = (
            sb.table("products_vector")
            .select(f"{PRODUCT_VECTOR_FK_COL}, content")
            .in_(PRODUCT_VECTOR_FK_COL, filtered_ids)
            .execute()
        )
        pv_rows = pv_resp.data or []
        pv_map = {r[PRODUCT_VECTOR_FK_COL]: r.get("content") for r in pv_rows}
        
        # 7) Cross-Encoder rerank + keyword bonus
        pairs: List[Tuple[str, str]] = []
        valid_ids: List[int] = []
        
        for pid in filtered_ids:
            content = pv_map.get(pid)
            if not content:
                continue
            valid_ids.append(pid)
            pairs.append((truncate_for_ce(query_text), truncate_for_ce(content)))
        
        if not pairs:
            print("[WARN] ë¸Œëœë“œ í•„í„°ë§ í›„ products_vector.contentê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
        
        ce_scores = ce.predict(pairs)
        
        reranked = []
        for pid, ce_score in zip(valid_ids, ce_scores):
            content = pv_map.get(pid, "")
            kwb = keyword_bonus(user_keywords, content)
            final_score = float(ce_score) + KW_BONUS_ALPHA * kwb
            p = prod_map.get(pid)
            
            reranked.append({
                "product_id": str(pid),
                "brand": p.get("brand"),
                "name": p.get("name"),
                "category_major": p.get("category_major"),
                "category_middle": p.get("category_middle"),
                "category_small": p.get("category_small"),
                "price_final": p.get("price_final"),
                "discount_rate": p.get("discount_rate"),
                "review_score": p.get("review_score"),
                "review_count": p.get("review_count"),
                "ce_score": float(ce_score),
                "kw_bonus": float(kwb),
                "final_score": float(final_score),
                "similarity": float(sim_map.get(pid, 0.0)),
            })
        
        reranked.sort(key=lambda r: r["final_score"], reverse=True)
        
        # 8) top_k ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        if top_k == 1:
            return reranked[0] if reranked else None
        else:
            return reranked[:top_k]
            
    except Exception as e:
        print(f"âŒ ìƒí’ˆ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


async def get_recommendation(request_data: Any) -> Dict[str, Any]:
    """
    Get recommendation using Cross-Encoder based system.
    """
    user_id = request_data.user_id
    intention = getattr(request_data, 'intention', None)
    user_data = request_data.user_data
    target_brands = getattr(request_data, 'target_brand', None)
    
    print(f"\nğŸ¯ ì¶”ì²œ ìš”ì²­ ìˆ˜ì‹ :")
    print(f"  - User ID: {user_id}")
    print(f"  - Intention: {intention}")
    print(f"  - Target Brands: {target_brands}")
    
    # Cross-Encoder ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ í˜¸ì¶œ
    recommendation = await recommend_product_with_brands(
        user_id=user_id,
        user_data=user_data,
        target_brands=target_brands if target_brands else [],
        top_k=1
    )
    
    if recommendation:
        print(f"  âœ… ìƒí’ˆ ì¶”ì²œ ì„±ê³µ: {recommendation['name']} (ID: {recommendation['product_id']})")
        print(f"  ğŸ“Š Score: ce={recommendation['ce_score']:.4f}, kw_bonus={recommendation['kw_bonus']:.3f}, final={recommendation['final_score']:.4f}")
        
        result = {
            "product_id": recommendation['product_id'],
            "product_name": recommendation['name'],
            "score": recommendation['final_score'],
            "reason": f"Cross-Encoder ì ìˆ˜: {recommendation['ce_score']:.4f}, í‚¤ì›Œë“œ ë§¤ì¹­: {recommendation['kw_bonus']:.3f}",
            "product_data": {
                "product_id": recommendation['product_id'],
                "brand": recommendation['brand'],
                "name": recommendation['name'],
                "category": {
                    "major": recommendation['category_major'],
                    "middle": recommendation['category_middle'],
                    "small": recommendation['category_small'],
                },
                "price": {
                    "original_price": recommendation['price_final'],
                    "discounted_price": recommendation['price_final'],
                    "discount_rate": recommendation['discount_rate'],
                },
                "review": {
                    "score": recommendation['review_score'],
                    "count": recommendation['review_count'],
                    "top_keywords": [],
                },
                "description_short": f"{recommendation['name']} - {recommendation['brand']}",
            }
        }
        return result
    
    # ì¶”ì²œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    print("  âš ï¸ ì¶”ì²œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
    return {
        "product_id": "UNKNOWN",
        "product_name": "ì¶”ì²œ ì‹¤íŒ¨",
        "score": 0.0,
        "reason": "ìƒí’ˆ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
    }


